# Spatia: Video Generation with Updatable Spatial Memory

## Paper Metadata

**Title:** Spatia: Video Generation with Updatable Spatial Memory

**Authors:**
- Jinjing Zhao¹*
- Fangyun Wei²*
- Zhening Liu³
- Hongyang Zhang⁴
- Chang Xu¹†
- Yan Lu²†

**Affiliations:**
1. The University of Sydney
2. Microsoft Research
3. HKUST
4. University of Waterloo

**Published:** NeurIPS 2025 (likely - arXiv:2512.15716)

**arXiv:** arXiv:2512.15716v1 [cs.CV]

**Date:** December 17, 2025

**Project Page:** https://zhaojingjing713.github.io/Spatia/

---

## Core Thesis

Video generation models struggle with long-term spatial and temporal consistency because video signals are dense and high-dimensional. Unlike LLMs that can attend to all historical tokens, video models cannot directly model entire sequences of spatio-temporal tokens.

**Problem:** A 5-second 480P video at 24 FPS = 36,000 spatio-temporal tokens. Including more context dramatically increases computational and memory demands.

**Solution:** Maintain an explicit 3D scene point cloud as persistent spatial memory. Iteratively:
1. Generate video clips conditioned on the scene point cloud
2. Update the point cloud using visual SLAM

This enables:
- Dynamic-static disentanglement
- Spatially consistent multi-view generation
- Explicit camera control
- 3D-aware interactive editing

---

## Key Innovations

### 1. Explicit Spatial Memory Mechanism

Unlike LLM KV-cache compression, Spatia uses a 3D scene point cloud as geometric memory:
- Estimated from initial image using MapAnything
- Updated iteratively via visual SLAM
- Provides dense spatial conditioning without massive token counts

### 2. Dynamic-Static Disentanglement

- Static scene → stored in point cloud (spatial memory)
- Dynamic entities → generated by video model
- Combines benefits of static scene generation + dynamic video generation

### 3. View-Specific Point Cloud Rendering

- Given camera poses + point cloud → render view-specific sequences
- Mirrors 3DGS rendering process
- Enables geometrically grounded camera control

### 4. Reference Frame Retrieval

- Select spatially relevant frames from candidate set
- Based on 3D IoU between point clouds
- Provides additional context for consistency

---

## Method Details

### Problem Formulation

Multi-modal conditional generation:
- Text instructions
- Spatial memory (3D point cloud)
- Temporal context (previous frames)

Two stages (iterated):
1. Generate video clip conditioned on multi-modal inputs
2. Update spatial memory with newly generated content

### Training Data

- RealEstate dataset (40K videos)
- SpatialVID dataset (HD subset, 10K videos)
- Resolution: 720P

### Architecture

**Backbone:** Wan2.2 video diffusion transformer (5B params)

**Key Components:**
- Video encoder for spatio-temporal tokens
- Scene projection encoder (point cloud → 2D projection → tokens)
- Reference frame encoder
- Text encoder (from Wan2.2)
- 8 network blocks: each has 1 ControlNet block + 4 main blocks

**Training:**
- Stage 1: Train ControlNet blocks (8,000 iterations, LR 1e-5)
- Stage 2: Fine-tune main blocks with LoRA (rank=64, 5,000 iterations, LR 1e-4)
- Batch size: 64 on 64× AMD MI250 GPUs

### Inference Process

```
Iteration 1:
1. User provides initial image
2. Estimate initial 3D scene point cloud (MapAnything)
3. Specify text instruction + camera path
4. Render projection video along camera path
5. Generate clip-1

Iteration n>1:
1. Update spatial memory using all previously generated frames (MapAnything)
2. User specifies new text + camera path
3. Retrieve reference frames (spatially relevant)
4. Generate clip-n conditioned on previous clip + projection + references
```

---

## Algorithms

### Algorithm 1: Reference Frame Retrieval

```
Input: Target frames {T}ₙ, candidate frames {C}ₒ,
       view-specific scene point clouds {S_T}ₙ, {S_C}ₒ,
       threshold ε, max reference frames K

Output: Retrieved reference frames {R}ₖ

1. For each target frame Tᵢ in {T}ₙ:
   a. For each candidate frame Cⱼ in {C}ₒ:
      - Compute spatial overlap: s(Tᵢ, Cⱼ) = 3DIoU(S_Tᵢ, register(S_Cⱼ, S_Tᵢ))
   b. Select candidate with max overlap if s > ε
2. Return up to K retrieved frames
```

### Dynamic-Static Disentanglement (Inference)

```
1. Apply SAM2 to track/segment dynamic entities in initial image
2. Record segmentation masks
3. Exclude dynamic regions when updating point cloud with MapAnything
4. Generate video with dynamic entities while keeping point cloud static
```

### Closed-Loop Generation

- Camera trajectory returns to initial viewpoint
- Compare final frame to initial image
- Tests spatial memory effectiveness

---

## Network Architecture Details

### Token Extraction

| Input Type | Encoder | Output |
|------------|---------|--------|
| Target frames {T}ₙ | Wan2.2 video encoder | X_T (spatio-temporal tokens) |
| Preceding frames {P}ₘ | Wan2.2 video encoder | X_P |
| Reference frames {R}ₖ | Wan2.2 video encoder | X_R (per-frame, concatenated) |
| Scene projections | Video encoder | X_{S_T}, X_{S_P} |
| Text | Wan2.2 text encoder | X_T (text tokens) |

### Block Structure

Each block:
- 1 ControlNet block (processes scene projection tokens)
- 4 Main blocks (Wan2.2 architecture)
  - Self-attention
  - Cross-attention (text as K, V)
  - FFN

ControlNet outputs → added to main block outputs for scene conditioning.

### Training Objective

Flow Matching:
```
L = E_{t,x₀,X_T}[||v_t - u_t||²]
```
where:
- t ~ logit-normal distribution
- x₀ ~ N(0, I) (pure noise)
- x_t = (1-t)x₀ + tX_T (linear interpolation)
- v_t = predicted velocity
- u_t = ground truth velocity

---

## Results

### WorldScore Benchmark

| Method | Average | Static | Dynamic | Camera Ctrl | 3D Const |
|--------|---------|--------|---------|-------------|-----------|
| WonderJourney | 54.19 | 63.75 | 44.63 | 84.60 | 35.54 |
| InvisibleStitch | 51.95 | 61.12 | 42.78 | 93.20 | 29.53 |
| WonderWorld | 61.79 | 72.69 | 50.88 | 92.98 | 71.25 |
| Voyager | 66.08 | 77.62 | 54.53 | 85.95 | 68.92 |
| VideoCrafter2 | 50.03 | 52.57 | 47.49 | 28.92 | 72.46 |
| CogVideoX-I2V | 60.64 | 62.15 | 59.12 | 38.27 | 36.73 |
| Wan2.1 | 55.21 | 57.56 | 52.85 | 23.53 | 45.44 |
| **Spatia (Ours)** | **69.73** | **72.63** | **66.82** | **75.66** | **69.95** |

### RealEstate Test Set

| Method | PSNR ↑ | SSIM ↑ | LPIPS ↓ |
|--------|--------|--------|---------|
| SEVA | 13.07 | 0.515 | 0.445 |
| VMem | 14.62 | 0.522 | 0.426 |
| ViewCrafter | 15.78 | 0.580 | 0.396 |
| FlexWorld | 16.25 | 0.593 | 0.370 |
| Voyager | 17.79 | 0.636 | 0.297 |
| **Spatia** | **18.58** | **0.646** | **0.254** |

### Memory Mechanism Evaluation (Closed-Loop)

| Method | PSNR_C ↑ | SSIM_C ↑ | LPIPS_C ↓ | Match Acc ↑ |
|--------|----------|-----------|------------|--------------|
| ViewCrafter | 14.79 | 0.481 | 0.365 | 0.447 |
| FlexWorld | 12.20 | 0.428 | 0.598 | 0.377 |
| Voyager | 17.66 | 0.540 | 0.380 | 0.507 |
| **Spatia** | **19.38** | **0.579** | **0.213** | **0.698** |

### Ablation Studies

**Scene Video + Reference Frames:**

| Scene Video | Reference Frames | Camera Ctrl | PSNR_C | SSIM_C | LPIPS_C |
|-------------|-----------------|-------------|--------|---------|---------|
| ✗ | ✗ | 58.81 | 15.55 | 0.444 | 0.379 |
| ✓ | ✗ | 80.13 | 17.18 | 0.500 | 0.295 |
| ✗ | ✓ | 61.38 | 15.64 | 0.444 | 0.393 |
| ✓ | ✓ | 84.47 | 19.38 | 0.579 | 0.213 |

**Number of Reference Frames:**

| #Refs | PSNR_C | SSIM_C | LPIPS_C | Match Acc |
|-------|--------|--------|---------|-----------|
| 1 | 17.50 | 0.537 | 0.284 | 0.592 |
| 3 | 17.85 | 0.540 | 0.275 | 0.606 |
| 5 | 18.48 | 0.556 | 0.248 | 0.640 |
| 7 | 19.38 | 0.579 | 0.213 | 0.698 |

**Point Cloud Density:**

| Cube Side (m) | PSNR | SSIM | LPIPS |
|--------------|------|------|-------|
| 0.01 | 18.58 | 0.646 | 0.254 |
| 0.03 | 17.10 | 0.614 | 0.313 |
| 0.05 | 16.35 | 0.596 | 0.349 |
| 0.07 | 15.97 | 0.585 | 0.370 |

---

## Applications

### (a) Dynamic-Static Disentanglement
- Static scene stored in point cloud memory
- Dynamic entities generated while maintaining scene consistency

### (b) Spatially Consistent Multi-View Generation
- Generate diverse videos of same location from different viewpoints
- Consistent spatial structure across views

### (c) Explicit Camera Control
- Specify camera trajectory as 3D path
- Render point cloud along trajectory → conditioning signal
- More explicit than latent camera encoding

### (d) 3D-Aware Interactive Editing
- Modify point cloud before generation
- Remove/add objects, change colors
- Edits reflected in generated videos

---

## Key Findings

1. **Spatial memory enables long-horizon consistency**
   - Closed-loop generation: final frame matches initial (PSNR 19.38)
   - Without memory: significant drift

2. **Reference frames + scene projection both matter**
   - Combination outperforms either alone
   - Scene projection: 58→84 camera control score
   - Reference frames: additional 5% improvement

3. **Point cloud density trades off memory vs quality**
   - Denser = better quality, more memory
   - 0.01m cube side optimal for RealEstate

4. **Dynamic-static disentanglement works**
   - Generate moving entities while preserving static scene
   - SAM2 segmentation for dynamic masking

---

## Relation to Hydrogen

### Spatial Memory for Rendering

Spatia's point cloud memory pattern could inform Hydrogen's rendering:
- Maintain spatial memory of scene geometry
- Update through frame iterations
- Enable consistent multi-view rendering

### Dynamic-Static Separation

- Static geometry → stored as data structure
- Dynamic elements → generated/rendered per frame
- Clean separation of concerns

### Camera Control via 3D Projection

- Render geometry from desired viewpoint
- Use projection as conditioning signal
- Enables precise camera control without learning

### Reference Frame Retrieval

- Find spatially relevant prior frames
- Use for consistency guidance
- Could apply to temporal coherence in animations

### MapAnything Integration

- Universal metric 3D reconstruction
- Could be used for scene estimation from any image
- Foundation for spatial memory initialization

---

## Technical Specifications

| Component | Specification |
|-----------|---------------|
| Backbone | Wan2.2 (5B params) |
| ControlNet | 8 blocks, parallel with main |
| LoRA | Rank = 64 |
| Training iterations | 8K (ControlNet) + 5K (LoRA) |
| Optimizer | AdamW |
| Learning rates | 1e-5 (ControlNet), 1e-4 (LoRA) |
| Batch size | 64 |
| GPUs | 64 × AMD MI250 |
| Default frames (iter 1) | 81 |
| Default frames (iter n>1) | 72 (conditioned on 9 prev) |
| Max reference frames | 7 |
| Default K | 7 |

---

## References (Paper-Specific)

### Key Methods Referenced
- [42] MapAnything - Universal feed-forward metric 3D reconstruction
- [87] Wan2.2 - Video generation backbone
- [115] ControlNet - Conditional control for diffusion
- [74] SAM2 - Segment anything 2
- [92] Dust3R - Geometric 3D vision
- [23] RoMa - Robust dense feature matching
- [56] Flow Matching - Training objective

### Benchmarks
- [122] RealEstate - View synthesis dataset
- [90] SpatialVID - Spatial video dataset
- [22] WorldScore - World generation benchmark

### Related Works
- VMem (2025) - Surfel-indexed view memory
- Voyager (2025) - Long-range world-consistent video diffusion
- ViewCrafter (2024) - Novel view synthesis
- FlexWorld (2025) - Progressively expanding 3D scenes

---

## Summary

Spatia introduces explicit spatial memory for video generation via 3D scene point clouds. Key contributions:

1. **Persistent spatial memory** - Point cloud maintained across generation iterations
2. **Dynamic-static disentanglement** - Static scene in memory, dynamic entities generated
3. **3D-aware camera control** - Render point cloud along trajectory for conditioning
4. **Reference frame retrieval** - Select spatially relevant prior frames

Results show significant improvements in spatial consistency (19.38 PSNR in closed-loop) and camera control (75.66 score) over state-of-the-art methods.

---

**Citation:**
```
@article{spatia2025,
  title={Spatia: Video Generation with Updatable Spatial Memory},
  author={Zhao, Jinjing and Wei, Fangyun and Liu, Zhening and Zhang, Hongyang and Xu, Chang and Lu, Yan},
  journal={arXiv preprint arXiv:2512.15716},
  year={2025}
}
```

---

*Document synthesized for Hydrogen research collection*
*Source: arXiv:2512.15716v1*
